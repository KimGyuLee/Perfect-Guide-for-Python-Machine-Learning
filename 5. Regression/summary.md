# Regression

## Intro
회귀는 여러 개의 독립변수와 한 개의 종속변수 간의 상관관계를 모델링하는 기법이다. 회귀는 지도 학습에서 가장 많이 활용되며, 주로 수치 데이터를 활용하는 기업의 데이터 분석에서 많이 사용된다.
머신러닝 회귀 예측의 핵심은 주어진 피처(독립 변수)와 결정 값 데이터(종속 변수) 기반에서 학습을 통해 최적의 회귀 계수를 찾아내는 것이다.

회귀는 회귀계수가 선형이면 선형 회귀, 선형이 아니면 비선형 회귀로 분류된다. 또한, 독립변수의 개수가 한 개이면 단일 회귀, 여러 개이면 다중 회귀로 나뉜다. 일반적으로 정형 데이터일 경우, 선형 회귀가 비선형회귀보다 예측 성능이 좋다.

최적의 회귀 모델을 만든다는 것은 전체 데이터의 잔차(오류 값) 합이 최소가 되는 모델을 만드는 것이다. 동시에 오류 값 합이 최소가 될 수 있는 최적의 회귀 계수를 찾는다는 의미이기도 하다.

Classification은 Category 값 (이산값)을 결과값으로 반환하고, Regression은 숫자값(연속값)을 반환한다.

* **선형 회귀의 종류**
  - 일반 선형 회귀 : 예측값과 실제값의 RSS (Residual Sum of Squares)를 최소화할 수 있도록 회귀 계수를 최적화하며, 규제(Regularization)를 적용하지 않는 모델이다.
  - 릿지(Ridge) : 릿지 회귀는 선형 회귀에 L2 규제를 추가한 회귀 모델이다.
  - 라쏘(Lasso) : 라쏘 회귀는 선형 회귀에 L1 규제를 적용한 방식이다.
  - 엘라스틱넷(ElasticNet) : L2, L1 규제를 함께 결합한 모델이다.
  - 로지스틱 회귀(Logistic Regression) : 로지스틱 회귀는 회귀라는 이름이 붙어 있지만, 분류에 사용되는 선형 모델이다. 이산값을 예측한다. 특히, 이진 분류에 많이 사용된다.  


## 1. 회귀 비용함수 RSS와 경사하강법   
* **RSS (Residual Sum of Squares)** : 오류 값의 제곱을 구해서 더하는 방식이다.  
  - RSS를 최소로 하는 회귀 계수를 학습을 통해서 찾는 것이 머신러닝 기반 회귀의 핵심 사항이다.
  - RSS는 회귀식의 독깁변수 X, 종속변수 Y가 중심 변수가 아니라 W 변수 (회귀 계수)가 중심 변수임을 인지하는 것이 매우 중요하다. (학습 데이터로 입력되는 독립 변수와 종속 변수는 RSS에서 모두 상수로 간주한다.)
  - 일반적으로 RSS는 학습 데이터의 건수로 나누어서 다음과 같이 정규화된 식으로 표현된다.
  - 회귀에서 이 RSS를 비용 함수(Cost function)로 지칭한다. 머신러닝은 RSS, 즉 비용을 계속 줄이는 방향으로 데이터를 학습한다. 최종적으로는 RSS가 더 이상 감소하지 않는 최소의 오류 값을 구하게 되고, 이 때 최적의 예측 모델이 만들어 진다.  

![RSS](https://user-images.githubusercontent.com/58073455/73270653-6d66db00-4222-11ea-8d70-6f315594a14f.PNG)

  
* **경사하강법 (Gradient Descent)** : W 파라미터의 개수가 적다면 고차원 방정식으로 비용 함수가 최소가 되는 W 변수값을 도출할 수 있지만, W 파라미터가 많으면 고차원 방정식을 동원하더라도 해결하기 어렵다. 경사 하강법은 이러한 고차원 방정식에 대한 문제를 해결해주면서 비용 함수 RSS를 최소화하는 방법을 직관적으로 제공하는 방식이다.  
  - 점진적으로 반복적인 계산을 통해 W 파라미터 값을 업데이트하면서 오류 값이 최소가 되는 W 파라미터를 구하는 방식이다.
  - 경사 하강법은 반복적으로 비용 함수의 반환 값, 즉 예측값과 실제 값의 차이가 작아지는 방향성을 가지고 W 파라미터를 지속해서 보정해 나간다.
  - 오류 값이 더 이상 작아지지 않으면 그 오류 값을 최소 비용으로 판단하고 그 때의 W 값을 최적 파라미터로 반환한다.
  - 경사 하강법의 핵심은 '어떻게 하면 오류가 작아지는 방향으로 W 값을 보정할 수 있는가'이다.  
  
* **어떻게 하면 오류가 작아지는 방향으로 W 값을 보정할 수 있는가?**
  - 답은 미분을 통해 최소값을 찾는 것이다. 최초 W 에서부터 미분을 적용한 뒤 이 미분 값이 계속 감소하는 방향으로 순차적으로 W를 업데이트 한다. 비용 함수가 포물선 형태의 2차 함수라고 가정한다면, 더 이상 미분된 1차 함수의 기울기가 감소하지 않는 지점을 비용 함수가 최소인 지점으로 간주하고 그때의 W 값을 반환한다.

![경사하강법](https://user-images.githubusercontent.com/58073455/73341606-537bd580-42c0-11ea-8918-ed0aa9a08cf6.PNG)

- w1, w0의 편미분 결과값을 반복적으로 보정하면서 w1, w0을 업데이트하면 비용함수 R(W)가 최소가 되는 w1, w0값을 구할 수 있다. 하지만 실제로는 위 편미분 값이 너무 클 수 있기 때문에 보정계수 ŋ를 곱하는데, 이를 학습률이라고 한다.

![경사하강법2](https://user-images.githubusercontent.com/58073455/73341861-d866ef00-42c0-11ea-8811-4fba06f9ae53.PNG)  


## 2. Linear Regression - 선형 회귀  

* **다중 공선성 문제**
  - 선형 회귀는 입력 피처의 독립성에 많은 영향을 받는다. 피처간의 상관관계가 높은 경우 분산이 커져서 오류에 민감해진다. 이러한 현상을 다중공선성(multi-collinearity) 문제라고 한다. 일반적으로 상관관계가 높은 피처가 많은 경우 독립적인 중요한 피처만 남기고 제거하거나 규제를 적용한다.  

* **회귀 평가지표**
![평가지표](https://user-images.githubusercontent.com/58073455/73361650-b6816280-42e8-11ea-9c5e-d23264587d07.PNG)  

* **선형 회귀 모델을 위한 데이터 변환**
  - 회귀 모델과 같은 선형 모델은 일반적으로 피처와 타겟값 간의 선형관계가 있다고 가정하고, 이러한 최적의 선형 함수를 찾아내 결과값을 예측한다. 또한 선형회귀 모델은 피처값과 타겟값의 분포가 정규 분포인 형태를 선호한다. (정규분포; 평균을 중심으로 종 모양으로 데이터 값이 분포된 형태)
  


## 3. Polynomial Regression - 다항 회귀  

- 회귀식이 독립변수의 단항식이 아닌 2차, 3차 방정식과 같은 다항식으로 표현되는 것을 말한다.
- 데이터 세트에서 피처 X에 대해 Y 값의 관계를 단순 선형 회귀 직선형으로 표현하는 것 보다 다항 회귀 곡선형으로 표현하는 것이 예측 성능이 더 높다.  

![다항회귀](https://user-images.githubusercontent.com/58073455/73363252-bf276800-42eb-11ea-91b6-d6bf2546ac8c.PNG)

- 다항 회귀는 선형 회귀이다. 회귀에서 선형/비선형 회귀를 나누는 기준은 '회귀 계수'가 선형/비선형인지에 따른 것이지 '독립변수' X의 선형/비선형 여부와는 무관하다. (X의 제곱을 새로운 변수 Z로 정의하면 Z에 대한 일차식이 된다.)

- 사이킷런은 다항회귀를 바로 API로 제공하지 않는다. 대신 PolynomialFeatures 클래스로 원본 단항 피처들을 다항 피처들로 변환한 데이터 세트에 LinearRegression 객체를 적용하여 다항회귀 기능을 제공한다.  


* **과소적합(Underfitting) 및 과대적합(Overfitting)**
  - degree에 따라 과소적합되기도 하고 과대적합되기도 한다.
  - 과소적합 : Bias(편향)가 높다. (편향되어있다. / 한 길만 간다.)
  - 과대적합 : Variance(분산)가 높다. (분산되어있다. / 여러 길로 간다.)
  - 편향이 높으면 분산은 낮아지고 (과소적합), 분산이 높으면 편향이 낮아진다(과대적합). 즉, 편향과 분산은 trade-off 관계로, well-fit한 모델을 만드는 것이 쉽지 않음을 의미한다.


## 4. Regularized Linear Regression - 규제 선형 회귀  

* **개념**
  - 지나치게 모든 데이터에 적합한 회귀식을 만들면 다항식이 복잡해지고 회귀 계수가 매우 크게 설정되면서 과대 적합이 된다. 이러한 회귀식은 테스트 데이터 셋에 대해서는 좋지 않은 예측 성능을 보인다. 따라서 회귀 모델은 적절히 데이터에 적합하면서도 회귀 계수가 기하급수적으로 커지는 것을 제어해야 한다.
  - 기존의 회귀식은 비용 함수가 RSS를 최소화하는 것만을 목표로 했다. RSS만 최소화하다보면 회귀 계수가 무한정 커지게 된다. 따라서 최적의 회귀 모델을 만들기 위해서는 비용함수에 RSS뿐만 아니라 회귀 계수 값의 크기를 제어하는 다른 구성 요소를 추가해야 한다. 이렇게 회귀 계수의 크기를 제어하는 튜닝 파라미터를 alpha라고 한다.  
  - alpha가 0이라면 비용 함수 식은 기존과 동일한 Min(RSS(W + 0)이 된다. 반면 alpha가 무한대라면 비용함수 식은 RSS(W)에 비해 alpha*||W|| 값이 너무 커지게 되므로 W 값을 0으로 만들어야 Cost가 최소로 만들 수 있다.
  - 즉, alpha 값을 크게 하면 비용 함수는 회귀 계수 W의 값을 작게 해서 과적합을 개선할 수 있으며, alpha 값을 작게 하면 회귀 계수 W의 값이 커져도 어느 정도 상쇄가 가능하므로 학습 데이터 적합을 개선할 수 있다.
  - RSS(W) 최소화에 초점을 맞춘다면 alpha 값을 작게, 회귀계수 W 감소에 초점을 맞춘다면 alpha 값을 크게하면 된다.
  - 이처럼 비용 함수에 alpha 값으로 패널티를 부여해 회귀 계수 값의 크기를 감소시켜 과적합을 개선하는 방식을 규제라고 부른다.

* **규제 방식 L1 & L2**
  - L2 규제는 W의 제곱에 대해 패널티를 부여하는 방식이다. L2 규제를 적용한 회귀를 릿지(Ridge) 회귀라고 한다.
  - L1 규제는 W의 절댓값에 대해 패널티를 부여하는 방식이다. L1 규제를 적용한 회귀를 라쏘(Lasso) 회귀라고 한다. L1 규제를 적용하면 영향력이 크지 않은 회귀 계수 값을 0으로 변환한다.
  - L1, L2 규제를 결합한 모델을 ElasticNet이라고 한다. 주로 피처가 많은 데이터 세트에 적용하며, L1 규제로 피처의 개수를 줄임과 동시에 L2 규제로 계수 값으 크기를 조정한다.
  - L2 규제가 회귀 계수의 크기를 감소시키는 데 반해, L1 규제는 불필요한 회귀 계수를 급격하게 감소시켜 0으로 만들고 제거한다. L1 규제는 feature selection 기능을 수행한다.
  - 라쏘 회귀는 서로 상관관계가 높은 피처들의 경우에 이들 중에서 중요 피처만을 선택하고 다른 피처들은 모두 회귀 계수를 0으로 만든다. 이럴 경우 alpha 값에 따라 회귀 계수의 값이 급격하게 변동할 수 있기 때문에, 이를 완화하기 위해 L2 규제를 라쏘 회귀에 추가한 것 엘라스틱넷이다.

## 5. Logistic Regression - 로지스틱 회귀





## 6. 회귀 트리
